\relax 
\@nameuse{bbl@beforestart}
\providecommand\hyper@newdestlabel[2]{}
\providecommand{\transparent@use}[1]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{nielsen15}
\citation{fcholet17}
\citation{jahr15}
\citation{jahr15}
\providecommand \oddpage@label [2]{}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Convolutional Autoencoder Trigger Concept}{1}{section.1}\protected@file@percent }
\newlabel{chp:nn_tpg}{{1}{1}{Convolutional Autoencoder Trigger Concept}{section.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Neural network}{1}{subsection.1.1}\protected@file@percent }
\newlabel{sec:nn}{{1.1}{1}{Neural network}{subsection.1.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Simple neural network architecture. Figure taken from Ref. \cite  {jahr15}.\relax }}{1}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:schema_nn}{{1}{1}{Simple neural network architecture. Figure taken from Ref. \cite {jahr15}.\relax }{figure.caption.1}{}}
\citation{colah_nn}
\newlabel{eq:single_activation}{{1}{2}{Neural network}{equation.1.1}{}}
\newlabel{eq:vec_activation}{{2}{2}{Neural network}{equation.1.2}{}}
\citation{menshawy18}
\citation{menshawy18}
\citation{Goodfellow16}
\citation{jordan_ae}
\newlabel{eq:mse}{{3}{3}{Neural network}{equation.1.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.1}Autoencoder}{3}{subsubsection.1.1.1}\protected@file@percent }
\newlabel{sec:ae}{{1.1.1}{3}{Autoencoder}{subsubsection.1.1.1}{}}
\citation{convBlock}
\citation{convBlock}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Schematic of a five-hidden-layer deep undercomplete autoencoder network. Figure taken from Ref. \cite  {menshawy18}.\relax }}{4}{figure.caption.2}\protected@file@percent }
\newlabel{fig:autoenc}{{2}{4}{Schematic of a five-hidden-layer deep undercomplete autoencoder network. Figure taken from Ref. \cite {menshawy18}.\relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.2}Convolutional Neural Network}{4}{subsubsection.1.1.2}\protected@file@percent }
\newlabel{sec:cnn}{{1.1.2}{4}{Convolutional Neural Network}{subsubsection.1.1.2}{}}
\citation{lecun_mnist}
\citation{nielsen15}
\citation{wiki_mnist}
\citation{wiki_mnist}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Working principle of convolutional layers. One pixel value of the resulting feature map is a weighted sum of an image patch with the kernel values. Figure taken from Ref. \cite  {convBlock}.\relax }}{5}{figure.caption.3}\protected@file@percent }
\newlabel{fig:conv}{{3}{5}{Working principle of convolutional layers. One pixel value of the resulting feature map is a weighted sum of an image patch with the kernel values. Figure taken from Ref. \cite {convBlock}.\relax }{figure.caption.3}{}}
\newlabel{eq:conv}{{4}{5}{Convolutional Neural Network}{equation.1.4}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.1.3}MNIST dataset}{5}{subsubsection.1.1.3}\protected@file@percent }
\newlabel{sec:mnist}{{1.1.3}{5}{MNIST dataset}{subsubsection.1.1.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Subset of the MNIST handwritten digit dataset. Figure taken from Ref. \cite  {wiki_mnist}.\relax }}{6}{figure.caption.4}\protected@file@percent }
\newlabel{fig:mnist}{{4}{6}{Subset of the MNIST handwritten digit dataset. Figure taken from Ref. \cite {wiki_mnist}.\relax }{figure.caption.4}{}}
\citation{trig_algos}
\citation{trig_algos}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Concept studies}{7}{subsection.1.2}\protected@file@percent }
\newlabel{sec:concept}{{1.2}{7}{Concept studies}{subsection.1.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Increase of trigger rate rate as a function of offline threshold for the three different compression algorithms of the ECON-T chip: Threshold, Super Trigger Cell, BestChoice. These algorithms have been presented in Sec.~\ref  {subsec:tpg}. Two further variants Coarse BestChoice first applies sorting and then selects the N largest values, BC+STC uses BestChoice in the ECAL and Super Trigger Cells in the HCAL, based on the observation that BestChoice works best for EM showers (small objects) and STC best for jets (larger objects). Table taken from Ref. \cite  {trig_algos}.\relax }}{8}{figure.caption.5}\protected@file@percent }
\newlabel{fig:trig_algo}{{5}{8}{Increase of trigger rate rate as a function of offline threshold for the three different compression algorithms of the ECON-T chip: Threshold, Super Trigger Cell, BestChoice. These algorithms have been presented in Sec.~\ref {subsec:tpg}. Two further variants Coarse BestChoice first applies sorting and then selects the N largest values, BC+STC uses BestChoice in the ECAL and Super Trigger Cells in the HCAL, based on the observation that BestChoice works best for EM showers (small objects) and STC best for jets (larger objects). Table taken from Ref. \cite {trig_algos}.\relax }{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.1}Elementary encoder unit}{9}{subsubsection.1.2.1}\protected@file@percent }
\newlabel{sec:generic_enc}{{1.2.1}{9}{Elementary encoder unit}{subsubsection.1.2.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Illustration of a neural network based HGCAL trigger primitive generator. Trigger cell data from three ROCs is sent to one ECON-T, where the data is concatenated and encoded to the latent representation of a module. This latent space is sent together with 20 other ECON-T encodings to the Stage-1 encoder, which, again, concatenates its inputs and encodes it. Stage-1 encodings are sent together with 29 other Stage-1 encodings to Stage-2, which concatenates its inputs and encodes it. The latent space of Stage-2 is the trigger primitive. ROC and ECON-T compression will be implemented in hardware, while Stage-1 and Stage-2 is implemented in firmware. Corresponding decoders will be implemented in software in order to train the network and monitor the outputs of each stage during inference.\relax }}{10}{figure.caption.6}\protected@file@percent }
\newlabel{fig:conc_arch}{{6}{10}{Illustration of a neural network based HGCAL trigger primitive generator. Trigger cell data from three ROCs is sent to one ECON-T, where the data is concatenated and encoded to the latent representation of a module. This latent space is sent together with 20 other ECON-T encodings to the Stage-1 encoder, which, again, concatenates its inputs and encodes it. Stage-1 encodings are sent together with 29 other Stage-1 encodings to Stage-2, which concatenates its inputs and encodes it. The latent space of Stage-2 is the trigger primitive. ROC and ECON-T compression will be implemented in hardware, while Stage-1 and Stage-2 is implemented in firmware. Corresponding decoders will be implemented in software in order to train the network and monitor the outputs of each stage during inference.\relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Illustration of convolutional autoencoder model. The encoder takes a $28\times 28$ input image, produces a $4\times 4$ latent space from which the decoder tries to reproduce the original image.\relax }}{11}{figure.caption.7}\protected@file@percent }
\newlabel{fig:cae1}{{7}{11}{Illustration of convolutional autoencoder model. The encoder takes a $28\times 28$ input image, produces a $4\times 4$ latent space from which the decoder tries to reproduce the original image.\relax }{figure.caption.7}{}}
\newlabel{fig:FCAE_expl}{{8a}{13}{Subfigure 8a}{subfigure.8.1}{}}
\newlabel{sub@fig:FCAE_expl}{{(a)}{a}{Subfigure 8a\relax }{subfigure.8.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Examples of input images (first row), their latent space representation (middle row) and their reconstruction from the latent space (last row) for: (a) The fully convolutional autoencoder (FCAE) and (b) the convolutional autoencoder (CAE). The training progression of (b) can be seen in Fig.~\ref  {fig:cae16_train}\relax }}{13}{figure.caption.8}\protected@file@percent }
\newlabel{fig:CAE_expl}{{8}{13}{Examples of input images (first row), their latent space representation (middle row) and their reconstruction from the latent space (last row) for: (a) The fully convolutional autoencoder (FCAE) and (b) the convolutional autoencoder (CAE). The training progression of (b) can be seen in Fig.~\ref {fig:cae16_train}\relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Progression of the MSE loss on the train set (blue) and the test/validation set (orange) for CAE with 16 latent dimensions and \SI {1000} training epochs.\relax }}{14}{figure.caption.9}\protected@file@percent }
\newlabel{fig:cae16_train}{{9}{14}{Progression of the MSE loss on the train set (blue) and the test/validation set (orange) for CAE with 16 latent dimensions and \SI {1000} training epochs.\relax }{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Reconstruction loss after 1000 training epochs as a function of inverse compression ratio of the CAE. Each data point corresponds to a five-hidden-layer deep undercomplete convolutional autoencoder trained on the MNIST handwritten digit dataset.\relax }}{14}{figure.caption.10}\protected@file@percent }
\newlabel{fig:latent_dim}{{10}{14}{Reconstruction loss after 1000 training epochs as a function of inverse compression ratio of the CAE. Each data point corresponds to a five-hidden-layer deep undercomplete convolutional autoencoder trained on the MNIST handwritten digit dataset.\relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.2}Split encoding}{15}{subsubsection.1.2.2}\protected@file@percent }
\newlabel{sec:latent_concat}{{1.2.2}{15}{Split encoding}{subsubsection.1.2.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.3}Invariance in $\bm  {\phi }$}{15}{subsubsection.1.2.3}\protected@file@percent }
\newlabel{sec:inv_phi}{{1.2.3}{15}{Invariance in \texorpdfstring {$\bm {\phi }$}{p}}{subsubsection.1.2.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Illustration of 4-Split model. Four similar encoding units with individual parameters produce four $2\times 2$ latent spaces, which are concatenated and fed to the same decoder architecture from the baseline model described in Sec. \ref  {sec:generic_enc}.\relax }}{16}{figure.caption.11}\protected@file@percent }
\newlabel{fig:4split}{{11}{16}{Illustration of 4-Split model. Four similar encoding units with individual parameters produce four $2\times 2$ latent spaces, which are concatenated and fed to the same decoder architecture from the baseline model described in Sec. \ref {sec:generic_enc}.\relax }{figure.caption.11}{}}
\newlabel{fig:f4s_expl}{{12a}{17}{Subfigure 12a}{subfigure.12.1}{}}
\newlabel{sub@fig:f4s_expl}{{(a)}{a}{Subfigure 12a\relax }{subfigure.12.1}{}}
\newlabel{fig:f4s_expl2}{{12b}{17}{Subfigure 12b}{subfigure.12.2}{}}
\newlabel{sub@fig:f4s_expl2}{{(b)}{b}{Subfigure 12b\relax }{subfigure.12.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Examples of input images (first row), their latent space representation (middle row) and their reconstruction from latent space (last row) for: (a) Fully convolutional 4-Split (F4S) and (b) 4-Split (4S) models.\relax }}{17}{figure.caption.12}\protected@file@percent }
\newlabel{fig:4split_2}{{12}{17}{Examples of input images (first row), their latent space representation (middle row) and their reconstruction from latent space (last row) for: (a) Fully convolutional 4-Split (F4S) and (b) 4-Split (4S) models.\relax }{figure.caption.12}{}}
\newlabel{fig:latent4free}{{13a}{18}{Subfigure 13a}{subfigure.13.1}{}}
\newlabel{sub@fig:latent4free}{{(a)}{a}{Subfigure 13a\relax }{subfigure.13.1}{}}
\newlabel{fig:latent4tied}{{13b}{18}{Subfigure 13b}{subfigure.13.2}{}}
\newlabel{sub@fig:latent4tied}{{(b)}{b}{Subfigure 13b\relax }{subfigure.13.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Latent space activation for same image fragment for 4-split with (a) individual weights and (b) shared weights.\relax }}{18}{figure.caption.13}\protected@file@percent }
\newlabel{fig:latents4}{{13}{18}{Latent space activation for same image fragment for 4-split with (a) individual weights and (b) shared weights.\relax }{figure.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Examples of input images (first row), their latent space representation (middle row) and their reconstruction from latent space (last row) for the 4-Split model with shared weights.\relax }}{18}{figure.caption.14}\protected@file@percent }
\newlabel{fig:4s_shared}{{14}{18}{Examples of input images (first row), their latent space representation (middle row) and their reconstruction from latent space (last row) for the 4-Split model with shared weights.\relax }{figure.caption.14}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.4}Partial encoding}{19}{subsubsection.1.2.4}\protected@file@percent }
\newlabel{sec:part_enc}{{1.2.4}{19}{Partial encoding}{subsubsection.1.2.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Example of $28\times 28$ digits randomly positioned inside a larger $56\times 56$ image of zeros. The digit is translated in integer steps in the range $0\rightarrow 27$ for x- and y-axes.\relax }}{19}{figure.caption.15}\protected@file@percent }
\newlabel{fig:rand_pos}{{15}{19}{Example of $28\times 28$ digits randomly positioned inside a larger $56\times 56$ image of zeros. The digit is translated in integer steps in the range $0\rightarrow 27$ for x- and y-axes.\relax }{figure.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Illustration of the 16-Split model. The 16 encoder units have the same architecture as the 4-Split encoders, generating a four unit latent space from a $14\times 14$ input image patch. Thus, the decoder input latent space consists of $64$ ($16\times 4$) values, from which the original $56\times 56$ image is reconstructed. The inverse compression ratio of the 16-Split is the same (0.02) as those of the 4-Split (4S) and the baseline (CAE) model.\relax }}{20}{figure.caption.16}\protected@file@percent }
\newlabel{fig:16s}{{16}{20}{Illustration of the 16-Split model. The 16 encoder units have the same architecture as the 4-Split encoders, generating a four unit latent space from a $14\times 14$ input image patch. Thus, the decoder input latent space consists of $64$ ($16\times 4$) values, from which the original $56\times 56$ image is reconstructed. The inverse compression ratio of the 16-Split is the same (0.02) as those of the 4-Split (4S) and the baseline (CAE) model.\relax }{figure.caption.16}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.5}Trigger primitive information}{20}{subsubsection.1.2.5}\protected@file@percent }
\newlabel{sec:tp_inf}{{1.2.5}{20}{Trigger primitive information}{subsubsection.1.2.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Examples of input images (first row), their latent space representation (middle row) and their reconstruction from latent space (last row) for the 16-Split model with shared weights.\relax }}{21}{figure.caption.17}\protected@file@percent }
\newlabel{fig:expl_16s}{{17}{21}{Examples of input images (first row), their latent space representation (middle row) and their reconstruction from latent space (last row) for the 16-Split model with shared weights.\relax }{figure.caption.17}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Training summary for different models, including number of trainable parameters for the entire model (encoder model), the training time per epoch and the saturated reconstruction loss (MSE) on the validation set after \SI {10000} epochs. The corresponding model architectures are detailed in Appendix~\ref  {app:nn_arch}, and their training loss curves are shown in Appendix~\ref  {app:nn_loss}.\relax }}{21}{table.caption.18}\protected@file@percent }
\newlabel{tab:sum}{{1}{21}{Training summary for different models, including number of trainable parameters for the entire model (encoder model), the training time per epoch and the saturated reconstruction loss (MSE) on the validation set after \SI {10000} epochs. The corresponding model architectures are detailed in Appendix~\ref {app:nn_arch}, and their training loss curves are shown in Appendix~\ref {app:nn_loss}.\relax }{table.caption.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces Examples of input images (first row), their latent space representation (middle row) and their the predicted classification (red) from latent space compared to the true class (blue) of a digit (last row) for the 16-Split with classification head (16S-C).\relax }}{22}{figure.caption.19}\protected@file@percent }
\newlabel{fig:16s_class}{{18}{22}{Examples of input images (first row), their latent space representation (middle row) and their the predicted classification (red) from latent space compared to the true class (blue) of a digit (last row) for the 16-Split with classification head (16S-C).\relax }{figure.caption.19}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces Examples of input images (first row), their latent space representation (middle row) and their the predicted regression of position (red circle) from latent space compared to the true position (blue circumference) of the center of a digit (last row) for the 16-Split with regression head (16S-R). Above the prediction, the euclidean distance, $\Delta r$, between truth and prediction is given.\relax }}{23}{figure.caption.20}\protected@file@percent }
\newlabel{fig:16reg}{{19}{23}{Examples of input images (first row), their latent space representation (middle row) and their the predicted regression of position (red circle) from latent space compared to the true position (blue circumference) of the center of a digit (last row) for the 16-Split with regression head (16S-R). Above the prediction, the euclidean distance, $\Delta r$, between truth and prediction is given.\relax }{figure.caption.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces Examples for 16-Split trained with classification and regression heads at the same time. Examples (first row), latent spaces (second row), classification probability distribution (third row), and regression results (last row).\relax }}{25}{figure.caption.21}\protected@file@percent }
\newlabel{fig:16s_class_regr}{{20}{25}{Examples for 16-Split trained with classification and regression heads at the same time. Examples (first row), latent spaces (second row), classification probability distribution (third row), and regression results (last row).\relax }{figure.caption.21}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Comparison of performance for different ways to train the 16-Split model for three different types of objectives: reconstruction (MSE loss), classification (CCE loss, binary accuracy metric), and regression (MAPE loss). The tasks without brackets in a row have been trained first, after which the encoder parameters were not changed anymore during training. The task in brackets comes from training the decoder on this (fixed) encoder. When more than one objective is trained at the same time, relative loss weights $LW$ had to be specified which have been chosen somewhat arbitrarily, and could probably be tuned to get better performance.\relax }}{26}{table.caption.22}\protected@file@percent }
\newlabel{tab:16sum}{{2}{26}{Comparison of performance for different ways to train the 16-Split model for three different types of objectives: reconstruction (MSE loss), classification (CCE loss, binary accuracy metric), and regression (MAPE loss). The tasks without brackets in a row have been trained first, after which the encoder parameters were not changed anymore during training. The task in brackets comes from training the decoder on this (fixed) encoder. When more than one objective is trained at the same time, relative loss weights $LW$ had to be specified which have been chosen somewhat arbitrarily, and could probably be tuned to get better performance.\relax }{table.caption.22}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces Asymmetric progression of two different losses (MAPE, CCE) as function of their contribution (encoder loss weight $LW$) to the global loss. A value of $-1$ corresponds to pure classification, $+1$ to pure regression.\relax }}{26}{figure.caption.23}\protected@file@percent }
\newlabel{fig:asym}{{21}{26}{Asymmetric progression of two different losses (MAPE, CCE) as function of their contribution (encoder loss weight $LW$) to the global loss. A value of $-1$ corresponds to pure classification, $+1$ to pure regression.\relax }{figure.caption.23}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {1.2.6}Information bottleneck}{27}{subsubsection.1.2.6}\protected@file@percent }
\newlabel{sec:inf_bott}{{1.2.6}{27}{Information bottleneck}{subsubsection.1.2.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Loss performance of different objectives after varying the inverse compression ratio $1/C$ (latent dimensions/input dimensions) of a model while keeping the rest of the architecture unchanged. Models are trained until saturation on either reconstruction, classification or regression alone (16S, 16S-C, 16S-R) or the encoders are trained on regression and classification, after which the encoder parameters are fixed and the reconstruction head is trained on top of this (16-RC). For each model the values corresponding to best classification accuracy are selected and summarized in the table. Models with larger capacity needed to be trained with an exponentially larger number of epochs in order to reach saturation level.\relax }}{27}{table.caption.25}\protected@file@percent }
\newlabel{tab:16latent}{{3}{27}{Loss performance of different objectives after varying the inverse compression ratio $1/C$ (latent dimensions/input dimensions) of a model while keeping the rest of the architecture unchanged. Models are trained until saturation on either reconstruction, classification or regression alone (16S, 16S-C, 16S-R) or the encoders are trained on regression and classification, after which the encoder parameters are fixed and the reconstruction head is trained on top of this (16-RC). For each model the values corresponding to best classification accuracy are selected and summarized in the table. Models with larger capacity needed to be trained with an exponentially larger number of epochs in order to reach saturation level.\relax }{table.caption.25}{}}
\newlabel{fig:sumMSE}{{22a}{28}{Subfigure 22a}{subfigure.22.1}{}}
\newlabel{sub@fig:sumMSE}{{(a)}{a}{Subfigure 22a\relax }{subfigure.22.1}{}}
\newlabel{fig:sumMAPE}{{22b}{28}{Subfigure 22b}{subfigure.22.2}{}}
\newlabel{sub@fig:sumMAPE}{{(b)}{b}{Subfigure 22b\relax }{subfigure.22.2}{}}
\newlabel{fig:sumCCE}{{22c}{28}{Subfigure 22c}{subfigure.22.3}{}}
\newlabel{sub@fig:sumCCE}{{(c)}{c}{Subfigure 22c\relax }{subfigure.22.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces Saturated loss value as a function of decoder input latent dimensions for the 16-Split encoder model trained with different heads and with different learning objectives. The curves correspond to the values summarized in Tab.~\ref  {tab:16latent}.\relax }}{28}{figure.caption.24}\protected@file@percent }
\newlabel{fig:sum16s}{{22}{28}{Saturated loss value as a function of decoder input latent dimensions for the 16-Split encoder model trained with different heads and with different learning objectives. The curves correspond to the values summarized in Tab.~\ref {tab:16latent}.\relax }{figure.caption.24}{}}
\citation{Steen2017}
\citation{Steen2017}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.3}Discussion}{29}{subsection.1.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces Example of test beam data for sensor occupancy due to a shower induced by a \SI {250}{GeV} electron \cite  {Steen2017}.\relax }}{30}{figure.caption.26}\protected@file@percent }
\newlabel{fig:shower_expl}{{23}{30}{Example of test beam data for sensor occupancy due to a shower induced by a \SI {250}{GeV} electron \cite {Steen2017}.\relax }{figure.caption.26}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {24}{\ignorespaces Confusion matrix showing prediction accuracy of true positives along the diagonal and accuracy of false positives in all other positions. The plot summarizes the classification results on the validation set for 16-split trained on regression and classification simultaneously. The x-axis shows the predicted digit (class) labels and the y-axis the actual labels.\relax }}{30}{figure.caption.27}\protected@file@percent }
\newlabel{fig:classif_distrib}{{24}{30}{Confusion matrix showing prediction accuracy of true positives along the diagonal and accuracy of false positives in all other positions. The plot summarizes the classification results on the validation set for 16-split trained on regression and classification simultaneously. The x-axis shows the predicted digit (class) labels and the y-axis the actual labels.\relax }{figure.caption.27}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {25}{\ignorespaces Box plot showing the median (orange) in its interquartile range (box) and outliers are marked by circles for 16-Split (trained on regression and classification simultaneously, and with a latent space of 128 decoder input dimensions) regression on the validation set: (a) The x-coordinate and (b) the y-coordinate. The positions are calculated from the center of the $28\times 28$ digit embedded in the $56\times 56$ image and cover the range (14,14) to (42, 42).\relax }}{31}{figure.caption.28}\protected@file@percent }
\newlabel{fig:residuals}{{25}{31}{Box plot showing the median (orange) in its interquartile range (box) and outliers are marked by circles for 16-Split (trained on regression and classification simultaneously, and with a latent space of 128 decoder input dimensions) regression on the validation set: (a) The x-coordinate and (b) the y-coordinate. The positions are calculated from the center of the $28\times 28$ digit embedded in the $56\times 56$ image and cover the range (14,14) to (42, 42).\relax }{figure.caption.28}{}}
